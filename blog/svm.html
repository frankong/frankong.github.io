<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6" crossorigin="anonymous">

    <title>Frank Ong</title>
  </head>
  <body>
    <div class="container">
      <div class="row my-4">
	<div class="col-xs-12 col-md-7 mx-auto">
	  <nav class="navbar navbar-expand-md navbar-light bg-light rounded border navbar-expand">
	    <a class="navbar-brand ms-3" href='../index.html'>
	      Frank Ong
	    </a>
	    <ul class="navbar-nav ms-auto">
	      <li class="nav-item">
		<a class="nav-link" href="index.html">Blog</a>
	      </li>
	      <li class="nav-item">
		<a class="nav-link" href="mailto:frankongh@gmail.com">Email</a>
	      </li>
	      <li class="nav-item">
		<a class="nav-link" href="https://github.com/frankong">Github</a>
	      </li>
	      <li class="nav-item me-3">
		<a class="nav-link" href="https://scholar.google.com/citations?user=zAM1TkoAAAAJ">Papers</a>
	      </li>
	    </ul>
	  </nav>
	</div>
      </div>

      <div class="row">
	<div class="col-11 col-md-7 mx-auto">
	  <center>
	    <h4>
	      Support Vector Machine (SVM)
	    </h4>
	  </center>
	  <center>
	    <p class="text-muted small">
	      Month DD, YYYY
	    </p>
	  </center>
	  <div>
	    <p>
	      The usual way of teaching support vector machine (SVM) is done through a binary classification point of view. There is a picture of pluses and minuses with a hyperplane separating them. A lot of emphasis is put on linear SVMs and margin. The generalization to the non-linear case is often done as an afterthought through the "kernel trick". 
	    </p>
	    <p>
	      Instead, I find it easier to understand SVM through a kernel regression point of view. We get the non-linear case directly. We get a picture of curves fitting through points. The optimization problem is simpler. And as you'll see, there's a clear connection to \(\ell 1\) regularized regression, with sparse elements corresponding to support vectors.
	    </p>
	    <p>
	      Our problem setup is as follows. We have training data points \((x_1, y_1), \ldots, (x_n, y_n).\) They can be scalars or vectors. And we want to fit a function \(f(x)\) such that \(f(x_i) \approx y_i.\)
	    </p>
	    <p>
	      Solving this problem directly is ill-posed because there are infinitely many ways to fit a curve to finite points. So we need to restrict the problem somehow. The kernel regression way is to constrain the function to be in a "kernel space". 
	    </p>
	    <p>
	      Concretely, given a positive semi-definite kernel \(\mathcal{K}(x, z)\), we can construct a corresponding reproducible kernel Hilbert space (RKHS) \(\mathbb{H}\). An RKHS is basically a function space that contains every sum of kernel functions like this,
	      \[
	      f(x) = \sum_{i = 1}^n \mathcal{K}(x, z_i),
	      \]
	      and their limits. This construction surprisingly induces an inner product \(\langle f, g \rangle\) and a norm \(\| f \|_\mathbb{H}\) in the function space.
	    </p>
	    <p>
	      RKHS has many interesting properties. One of them is that evaluating any function \(f\) in the space at any point \(x_0\) becomes an inner product operation:
	      \[
	      f(x_0) = \langle f, \mathcal{K}(x, x_0) \rangle
	      \]
	    </p>
	    <p>
	      And this basically transforms our function fitting problem to a linear inverse problem in the function space. Because \(f(x_i) \approx y_i\) now becomes \(\langle f, \mathcal{K}(x, x_i) \rangle \approx y_i\).
	    </p>
	    <p>
	      What's left is choosing a loss function. We choose a very particular one like this
	      \[
	      | f(x_i) - y_i | \leq \epsilon,
	      \]
	      where \(\epsilon\) is our approximation tolerance.
	    </p>
	    <p>
	      Putting things together, we have the following kernel regression problem:
	      \begin{align}
	      &\min_{f} &&\| f \|_{\mathbb{H}}\\
	      &\text{s.t.} && |f(x_i) - y_i| \leq \epsilon
	      \end{align}
	      for \(i = 1, \ldots, n\)
	    </p>
	    <p>
	      And that's it. This is an SVM. Some people call it <a href="https://alex.smola.org/papers/2004/SmoSch04.pdf">"support vector regression with hard margin"</a>. It is not the usual form people use, but it is still an SVM.
	    </p>
	    <p>
	      You can also see the connection to \(\ell 1\) regularized regression. The loss function we used is an \(\ell_\infty\) ball constraint. So the dual problem is an \(\ell 1\) problem. You can derive the dual problem with Lagrangian, and it becomes:
	      \[
	      \min_{\boldsymbol{\alpha}} \frac{1}{2} \boldsymbol{\alpha}^\top \mathbf{K} \boldsymbol{\alpha} - \boldsymbol{\alpha}^\top \mathbf{y} + \epsilon \| \boldsymbol{\alpha}\|_1
	      \]
	      where \(\boldsymbol{\alpha}\) is the dual variable. \(\mathbf{K}\) is the kernel matrix with \((\mathbf{K})_{ij} = \mathcal{K}(x_i, x_j)\), and \(\mathbf{y}\) is the vectorized output.
	    </p>
	    <p>
	      Finally, the primal and dual variables are related as follows:
	      \[
	      f(x) = \sum_{i = 1}^n \alpha_i \mathcal{K}(x, x_i)
	      \]
	  </div>
	</div>

	
      </div>
      <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js" integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf" crossorigin="anonymous"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </body>
</html>
