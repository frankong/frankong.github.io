<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6" crossorigin="anonymous">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-6CEYBYQ970"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-6CEYBYQ970');
    </script>
    <title>Support Vector Machine</title>
  </head>
  <body>
    <div class="container">
      <div class="row my-4">
	<div class="col-xs-12 col-md-7 mx-auto">
	  <nav class="navbar navbar-expand-md navbar-light bg-light rounded border navbar-expand">
	    <a class="navbar-brand ms-3" href='../index.html'>
	      Frank Ong
	    </a>
	    <ul class="navbar-nav ms-auto">
	      <li class="nav-item">
		<a class="nav-link" href="index.html">Blog</a>
	      </li>
	      <li class="nav-item">
		<a class="nav-link" href="mailto:frankongh@gmail.com">Email</a>
	      </li>
	      <li class="nav-item">
		<a class="nav-link" href="https://github.com/frankong">Github</a>
	      </li>
	      <li class="nav-item me-3">
		<a class="nav-link" href="https://scholar.google.com/citations?user=zAM1TkoAAAAJ">Papers</a>
	      </li>
	    </ul>
	  </nav>
	</div>
      </div>

      <div class="row">
	<div class="col-11 col-md-7 mx-auto">
	  <center>
	    <h4>
	      Support Vector Machine
	    </h4>
	  </center>
	  <center>
	    <p class="text-muted small">
	      Month DD, YYYY
	    </p>
	  </center>
	  <div>
	    <p>
	      The usual way of explaining support vector machines (SVM) is through  a binary classification point of view. There is a picture of pluses and minuses with a hyperplane separating them. A lot of time is spent on the margins and hyperplane in the linear case. The generalization to the non-linear case is almost done as an afterthought with the "kernel trick". 
	    </p>
	    <center>
	      <figure class="figure">
		<img src="../images/svm.png" class="figure-img img-fluid" alt="figure"/>
	      </figure>
	    </center>
	    <p>
	      Instead, I find it more intuitive to understand SVM through a kernel regression point of view. The picture becomes a curve fitting through data points. We get the non-linear case directly. The optimization problem is conceptually simpler. And there's a clear connection to \(\ell 1\) regularized regression, which links support vectors to sparsity.
	    </p>
	    <center>
	      <figure class="figure">
		<img src="../images/svr.png" class="figure-img img-fluid" alt="svr"/>
	      </figure>
	    </center>
	    <p>
	      Our problem setup is as follows. We have training data points \((x_1, y_1), \ldots, (x_n, y_n).\) For simplicity, everything is a scalar. And we want to fit a function \(f(x)\) such that \(f(x_i) \approx y_i.\)
	    </p>
	    <p>
	      There are infinitely many ways to fit a curve to finite points, so we need to somehow restrict the problem. The kernel regression way is to constrain the function to be in a "kernel space". At a high level, a kernel is a function that takes two inputs and outputs one scalar. A "kernel space" contains all linear combination of  kernel functions with one input fixed.
	    </p>
	    <p>
	      Concretely, given a <a class="link-secondary" href="https://en.wikipedia.org/wiki/Positive-definite_kernel">positive semi-definite kernel</a> \(\mathcal{K}(x, z)\), we can construct a corresponding reproducible kernel Hilbert space (RKHS) \(\mathbb{H}\). An RKHS is a function space that contains every sum of kernel functions like this,
	      \[
	      f(x) = \sum_{i = 1}^n \mathcal{K}(x, z_i),
	      \]
	      and their limits. This construction surprisingly induces an inner product \(\langle f, g \rangle\) and a norm \(\| f \|_\mathbb{H}\) in the function space.
	    </p>
	    <p>
	      RKHS has many interesting properties. One of them is that evaluating any function \(f\) in the space at any point \(x_0\) becomes an inner product operation:
	      \[
	      f(x_0) = \langle f, \mathcal{K}(x, x_0) \rangle
	      \]
	    </p>
	    <p>
	      And this basically transforms our function fitting problem to a linear inverse problem in the function space. Because \(f(x_i) \approx y_i\) now becomes \(\langle f, \mathcal{K}(x, x_i) \rangle \approx y_i\).
	    </p>
	    <p>
	      What's left is choosing a loss function. Kernel regression intersects with SVM when we pick a maximum deviation error loss as follow,
	      \[
	      | f(x_i) - y_i | \leq \epsilon,
	      \]
	      where \(\epsilon\) is our approximation tolerance.
	    </p>
	    <p>
	      Putting things together, we have the following kernel regression problem:
	      \begin{align}
	      &\min_{f} &&\| f \|_{\mathbb{H}}\\
	      &\text{s.t.} && |f(x_i) - y_i| \leq \epsilon
	      \end{align}
	      for \(i = 1, \ldots, n\)
	    </p>
	    <p>
	      And that's it. This is an SVM. Some people call it <a class="link-secondary" href="https://alex.smola.org/papers/2004/SmoSch04.pdf">"\(\epsilon\)-insensitive support vector regression"</a>. It is not the usual form, but it is still an SVM.
	    </p>
	    <p>
	      You can also see the connection to \(\ell 1\) regularized regression. The loss function we used is an \(\ell_\infty\) ball constraint. So its dual is an \(\ell 1\) penalty. You can derive the dual problem with Lagrangian, and it becomes:
	      \[
	      \min_{\boldsymbol{\alpha}} \frac{1}{2} \boldsymbol{\alpha}^\top \mathbf{K} \boldsymbol{\alpha} - \boldsymbol{\alpha}^\top \mathbf{y} + \epsilon \| \boldsymbol{\alpha}\|_1
	      \]
	      where \(\boldsymbol{\alpha}\) is the dual variable. \(\mathbf{K}\) is the kernel matrix with \((\mathbf{K})_{ij} = \mathcal{K}(x_i, x_j)\), and \(\mathbf{y}\) is the vectorized output.
	    </p>
	    <p>
	      And the primal and dual variables are related as follows:
	      \[
	      f(x) = \sum_{i = 1}^n \alpha_i \mathcal{K}(x, x_i)
	      \]
	    </p>
	    <p>
	      Our dual variable \(\boldsymbol{\alpha}\) is sparse because of the \(\ell 1\) regularized optimization. The training data \(x_i\) with non-zero coefficents are called support vectors.
	    </p>
	    
	  </div>
	</div>

	
      </div>
      <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js" integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf" crossorigin="anonymous"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </body>
</html>
