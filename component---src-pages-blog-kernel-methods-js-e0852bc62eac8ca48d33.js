(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{McE4:function(n,t,e){"use strict";e.d(t,"a",(function(){return r}));var a=e("q1tI"),i=e.n(a);function r(n){var t=n.children;return i.a.createElement("div",{className:"text-justify text-body"},t)}},YRlN:function(n,t,e){"use strict";e("eI33"),e("q1tI");function a(){var n=r(["(",")"],["\\(","\\)"]);return a=function(){return n},n}function i(){var n=r(["[","]"],["\\[","\\]"]);return i=function(){return n},n}function r(n,t){return t||(t=n.slice(0)),n.raw=t,n}t.a={block:function(n){return String.raw(i(),n.raw)},inline:function(n){return String.raw(a(),n.raw)}}},eI33:function(n,t,e){var a=e("XKFU"),i=e("aCFj"),r=e("ne8i");a(a.S,"String",{raw:function(n){for(var t=i(n.raw),e=r(t.length),a=arguments.length,l=[],o=0;e>o;)l.push(String(t[o++])),o<a&&l.push(String(arguments[o]));return l.join("")}})},"jJ/D":function(n,t,e){"use strict";e.d(t,"a",(function(){return r}));var a=e("q1tI"),i=e.n(a);function r(n){var t=n.children;return i.a.createElement("center",null,i.a.createElement("figure",{className:"figure"},i.a.createElement("img",{src:t,className:"figure-img img-fluid rounded"})))}},vWbJ:function(n,t,e){"use strict";e.r(t);var a=e("q1tI"),i=e.n(a),r=e("Bl7J"),l=e("WORS"),o=e("CfKq"),u=e("McE4"),c=(e("jJ/D"),e("/a2i")),f=e("YRlN");function s(){var n=L(["min_{alpha in mathbb{R}^n} \frac{1}{2} alpha^T K alpha - alpha^T y + epsilon | alpha |_1"],["\\min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{2} \\alpha^T K \\alpha - \\alpha^T y + \\epsilon \\| \\alpha \\|_1"]);return s=function(){return n},n}function h(){var n=L(["f"]);return h=function(){return n},n}function m(){var n=L(["f = sum_{i = 1}^n alpha_i  k(x, x_i)."],["f = \\sum_{i = 1}^n \\alpha_i  k(x, x_i)."]);return m=function(){return n},n}function p(){var n=L(["f"]);return p=function(){return n},n}function _(){var n=L(["min_{f in mathbb{F}} max_{alpha} \frac{1}{2}| f |_mathbb{F}^2 + sum_{i = 1}^n alpha_i (f(x_i) - y_i) - epsilon | alpha |_1"],["\\min_{f \\in \\mathbb{F}} \\max_{\\alpha} \\frac{1}{2}\\| f \\|_\\mathbb{F}^2 + \\sum_{i = 1}^n \\alpha_i (f(x_i) - y_i) - \\epsilon \\| \\alpha \\|_1"]);return _=function(){return n},n}function b(){var n=L(["alpha = alpha^+ - alpha^-"],["\\alpha = \\alpha^+ - \\alpha^-"]);return b=function(){return n},n}function d(){var n=L(["min_{f in mathbb{F}} max_{alpha^+ geq 0, alpha^- leq 0} \frac{1}{2}| f |_mathbb{F}^2 + sum_{i = 1}^n alpha^+_i (f(x_i) - y_i - epsilon) + sum_{i = 1}^n alpha^-_i (-f(x_i) + y_i - epsilon)"],["\\min_{f \\in \\mathbb{F}} \\max_{\\alpha^+ \\geq 0, \\alpha^- \\leq 0} \\frac{1}{2}\\| f \\|_\\mathbb{F}^2 + \\sum_{i = 1}^n \\alpha^+_i (f(x_i) - y_i - \\epsilon) + \\sum_{i = 1}^n \\alpha^-_i (-f(x_i) + y_i - \\epsilon)"]);return d=function(){return n},n}function g(){var n=L([void 0],["\\begin{align} &\\underset{f \\in \\mathbb{F}}{\\text{minimize}} &&\\| f \\|_\\mathbb{F} \\\\ &\\text{subject to} &&| f(x_i) - y_i | \\leq \\epsilon,~\\text{for}~i = 1, \\ldots, n \\end{align}"]);return g=function(){return n},n}function v(){var n=L(["alpha = K^{-1} y"],["\\alpha = K^{-1} y"]);return v=function(){return n},n}function x(){var n=L(["min_{alpha in mathbb{R}^n} \frac{1}{2} alpha^T K alpha - alpha^T y"],["\\min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{2} \\alpha^T K \\alpha - \\alpha^T y"]);return x=function(){return n},n}function y(){var n=L(["f"]);return y=function(){return n},n}function k(){var n=L(["f = sum_{i = 1}^n alpha_i  k(x, x_i)"],["f = \\sum_{i = 1}^n \\alpha_i  k(x, x_i)"]);return k=function(){return n},n}function w(){var n=L(["f"]);return w=function(){return n},n}function E(){var n=L(["f"]);return E=function(){return n},n}function S(){var n=L(["min_{f in mathbb{F}} max_{alpha in mathbb{R}^n} \frac{1}{2}| f |_mathbb{F}^2 + sum_{i = 1}^n alpha_i ( y_i - f(x_i) )"],["\\min_{f \\in \\mathbb{F}} \\max_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{2}\\| f \\|_\\mathbb{F}^2 + \\sum_{i = 1}^n \\alpha_i ( y_i - f(x_i) )"]);return S=function(){return n},n}function F(){var n=L([void 0],["\\begin{align} &\\underset{f \\in \\mathbb{F}}{\\text{minimize}} &&\\| f \\|_\\mathbb{F} \\\\ &\\text{subject to} &&f(x_i) = y_i,~\\text{for}~i = 1, \\ldots, n \\end{align}"]);return F=function(){return n},n}function I(){var n=L(["f(x) = langle f, k(x, cdot) \rangle"],["f(x) = \\langle f, k(x, \\cdot) \\rangle"]);return I=function(){return n},n}function K(){var n=L(["k(x, z)"]);return K=function(){return n},n}function R(){var n=L(["mathbb{F}"],["\\mathbb{F}"]);return R=function(){return n},n}function j(){var n=L(["langle k(x_i, cdot), k(x_j, cdot) \rangle = k(x_i, x_j)"],["\\langle k(x_i, \\cdot), k(x_j, \\cdot) \\rangle = k(x_i, x_j)"]);return j=function(){return n},n}function q(){var n=L(["f(x) = sum_{i = 1}^m alpha_i k(x, z_i)"],["f(x) = \\sum_{i = 1}^m \\alpha_i k(x, z_i)"]);return q=function(){return n},n}function M(){var n=L(["k(x, z)"]);return M=function(){return n},n}function z(){var n=L(["1"]);return z=function(){return n},n}function T(){var n=L(["0"]);return T=function(){return n},n}function H(){var n=L(["x"]);return H=function(){return n},n}function N(){var n=L(["y"]);return N=function(){return n},n}function J(){var n=L(["(x_1, y_1), ldots, (x_n, y_n)"],["(x_1, y_1), \\ldots, (x_n, y_n)"]);return J=function(){return n},n}function V(){var n=L(["l_infty"],["l_\\infty"]);return V=function(){return n},n}function L(n,t){return t||(t=n.slice(0)),n.raw=t,n}t.default=function(){return i.a.createElement(r.a,null,i.a.createElement(c.a,null,i.a.createElement(l.a,null,"Notes on Kernel Methods and Support Vector Machines"),i.a.createElement(o.a,null,"May 30th, 2020"),i.a.createElement(u.a,null,i.a.createElement("p",null,"In this post, I will give an overview of kernel methods and support vector machines (SVM). Most tutorials out there start with linear SVM for binary classification. Illustrations include hyperplanes separating points. Non-linear feature maps and the kernel trick are then introduced almost as an afterthought. I find this formulation confusing as the mental pictures from linear SVM are difficult to translate to the non-linear case."),i.a.createElement("p",null,"Instead, I will start with kernel methods within the reproducing kernel Hilbert space (RKHS) framework and talk about how support vector machines are simply kernel methods with ",f.a.inline(V())," error ball constraints."),i.a.createElement("h5",null,"Problem Setup: Supervised Machine Learning"),i.a.createElement("p",null,"In supervised machine learning, we are interested in an interpolation problem. Given many data pairs ",f.a.inline(J()),", we would like to output a new ",f.a.inline(N())," given a new ",f.a.inline(H()),". This is exactly interpolation, so the mental picture is something like this:"),i.a.createElement("p",null,"For classification, the only difference is that the output is discrete, say ",f.a.inline(T())," or ",f.a.inline(z()),". But the problem is still an interpolation problem:"),i.a.createElement("p",null,"In general, there are infinitely many solutions to interpolate between finitely many data points. But if we sample fine enough, and if the underlying distribution is somewhat regular, then intuitively we should be able to interpolate between points accurately. And one way of interpolating points is through kernel methods. We can construct an interpolating function by considering linear combinations of kernel functions sitting on the data points."),i.a.createElement("p",null,'In particular, RKHS looks at the space of all functions "span" by a positive definite kernel function ',f.a.inline(M()),". One way of constructing an RKHS is first considering the space of all functions  constructed with finite sums of the form ",f.a.inline(q()),", and defining the inner product as ",f.a.inline(j()),". Completing the space by including all limits of the finite sums forms an RKHS."),i.a.createElement("p",null,"Given an RKHS ",f.a.inline(R())," with a kernel function ",f.a.inline(K()),", function evaluation can be expressed as a linear operation: ",f.a.inline(I()),".",f.a.block(F())),i.a.createElement("p",null,"What is remarkable is that the optimization can be reduced to a finite-dimentional problem. In particular, we can use duality to obtain the following minimax problem:",f.a.block(S()),"Strong duality holds because the constraints are linear equalities. So we can switch the min-max and minimizes over ",f.a.inline(E()),". Setting the derivative with respect to ",f.a.inline(w())," to zero, we obtain:",f.a.block(k())),i.a.createElement("p",null,"This results in a function that is a linear combination of kernel functions sitted on the sampled data."),i.a.createElement("p",null,"Substituting ",f.a.inline(y())," into the objective function and rearranging, we obtain:",f.a.block(x()),"Hence, ",f.a.inline(v())),i.a.createElement("h5",null,"SVM"),i.a.createElement("p",null,f.a.block(g()),"Let us split the error constraint into positive and negative and look at the Lagrangian dual again:",f.a.block(d())),i.a.createElement("p",null,"Let ",f.a.inline(b()),", then we have,",f.a.block(_())),i.a.createElement("p",null,"Similar to exact equality, optimizing over ",f.a.inline(p())," results in",f.a.block(m()),"Substituting ",f.a.inline(h())," into the objective function and rearranging, we obtain:",f.a.block(s())))))}}}]);
//# sourceMappingURL=component---src-pages-blog-kernel-methods-js-e0852bc62eac8ca48d33.js.map