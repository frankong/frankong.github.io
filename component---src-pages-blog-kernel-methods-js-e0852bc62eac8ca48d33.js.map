{"version":3,"sources":["webpack:///./src/components/text.js","webpack:///./src/components/math.js","webpack:///./node_modules/core-js/modules/es6.string.raw.js","webpack:///./src/components/figure.js","webpack:///./src/pages/blog/kernel_methods.js"],"names":["Text","children","className","block","string","String","raw","inline","$export","toIObject","toLength","S","callSite","tpl","len","length","aLen","arguments","res","i","push","join","Figure","src","Math"],"mappings":"2FAAA,2DAEe,SAASA,EAAT,GAA2B,IAAZC,EAAW,EAAXA,SAC1B,OACH,yBAAKC,UAAU,0BACZD,K,qQCMW,KAACE,MAThB,SAAeC,GACX,OAAOC,OAAOC,IAAd,IAAsBF,EAAOE,MAQVC,OALvB,SAAgBH,GACZ,OAAOC,OAAOC,IAAd,IAAsBF,EAAOE,Q,qBCPjC,IAAIE,EAAU,EAAQ,QAClBC,EAAY,EAAQ,QACpBC,EAAW,EAAQ,QAEvBF,EAAQA,EAAQG,EAAG,SAAU,CAE3BL,IAAK,SAAaM,GAMhB,IALA,IAAIC,EAAMJ,EAAUG,EAASN,KACzBQ,EAAMJ,EAASG,EAAIE,QACnBC,EAAOC,UAAUF,OACjBG,EAAM,GACNC,EAAI,EACDL,EAAMK,GACXD,EAAIE,KAAKf,OAAOQ,EAAIM,OAChBA,EAAIH,GAAME,EAAIE,KAAKf,OAAOY,UAAUE,KACxC,OAAOD,EAAIG,KAAK,Q,oCCftB,2DAEe,SAASC,EAAT,GAA6B,IAAZrB,EAAW,EAAXA,SAC5B,OACH,gCACE,4BAAQC,UAAU,UACV,yBAAKqB,IAAKtB,EAAUC,UAAU,qC,woICK1B,qBACX,OACH,kBAAC,IAAD,KACE,kBAAC,IAAD,KACE,kBAAC,IAAD,4DACA,kBAAC,IAAD,uBACA,kBAAC,IAAD,KACE,gdAGA,+MAC6KsB,IAAKjB,OADlL,iCAGA,0EACA,mIACiGiB,IAAKjB,OADtG,wCAC8KiB,IAAKjB,OADnL,qBAC2MiB,IAAKjB,OADhN,uFAGA,0GACwEiB,IAAKjB,OAD7E,YAC4FiB,IAAKjB,OADjG,6DAGA,geAGA,oIACkGiB,IAAKjB,OADvG,oIACoPiB,IAAKjB,OADzP,4CAC+UiB,IAAKjB,OADpV,yFAGA,4CACUiB,IAAKjB,OADf,gCAC2DiB,IAAKjB,OADhE,sEAC+IiB,IAAKjB,OADpJ,SAEJiB,IAAKrB,MAFD,MAIA,oMAEJqB,IAAKrB,MAFD,4HAGgHqB,IAAKjB,OAHrH,iDAGyKiB,IAAKjB,OAH9K,4BAIJiB,IAAKrB,MAJD,MAMA,sIAGA,2CACSqB,IAAKjB,OADd,gEAEJiB,IAAKrB,MAFD,eAGGqB,IAAKjB,OAHR,MAKA,mCACA,2BACJiB,IAAKrB,MADD,2GAGJqB,IAAKrB,MAHD,MAKA,kCACAqB,IAAKjB,OADL,uBAEJiB,IAAKrB,MAFD,MAIA,yEACuCqB,IAAKjB,OAD5C,mBAEJiB,IAAKrB,MAFD,qBAGSqB,IAAKjB,OAHd,gEAIJiB,IAAKrB,MAJD","file":"component---src-pages-blog-kernel-methods-js-e0852bc62eac8ca48d33.js","sourcesContent":["import React from \"react\"\n\nexport default function Text({children}) {\n    return (\n\t<div className=\"text-justify text-body\">\n\t  {children}\n\t</div>\n    )\n}\n","import React from \"react\"\n\nfunction block(string) {\n    return String.raw`\\[${string.raw}\\]`\n}\n\nfunction inline(string) {\n    return String.raw`\\(${string.raw}\\)`\n}\n\n\nexport default {block, inline};\n","var $export = require('./_export');\nvar toIObject = require('./_to-iobject');\nvar toLength = require('./_to-length');\n\n$export($export.S, 'String', {\n  // 21.1.2.4 String.raw(callSite, ...substitutions)\n  raw: function raw(callSite) {\n    var tpl = toIObject(callSite.raw);\n    var len = toLength(tpl.length);\n    var aLen = arguments.length;\n    var res = [];\n    var i = 0;\n    while (len > i) {\n      res.push(String(tpl[i++]));\n      if (i < aLen) res.push(String(arguments[i]));\n    } return res.join('');\n  }\n});\n","import React from \"react\"\n\nexport default function Figure({children}) {\n    return (\n\t<center>\n\t  <figure className=\"figure\">\n      \t    <img src={children} className=\"figure-img img-fluid rounded\"/>\n\t  </figure>\n\t</center>\n    )\n}\n","import React from \"react\"\n\nimport Layout from \"../../components/layout\"\nimport Title from \"../../components/title\"\nimport Subtitle from \"../../components/subtitle\"\nimport Text from \"../../components/text\"\nimport Figure from \"../../components/figure\"\nimport Block from \"../../components/block\"\nimport Math from \"../../components/math\"\n\n\nexport default () => {\n    return (\n\t<Layout>\n\t  <Block>\n\t    <Title>Notes on Kernel Methods and Support Vector Machines</Title>\n\t    <Subtitle>May 30th, 2020</Subtitle>\n\t    <Text>\n\t      <p>\n\t\tIn this post, I will give an overview of kernel methods and support vector machines (SVM). Most tutorials out there start with linear SVM for binary classification. Illustrations include hyperplanes separating points. Non-linear feature maps and the kernel trick are then introduced almost as an afterthought. I find this formulation confusing as the mental pictures from linear SVM are difficult to translate to the non-linear case.\n\t      </p>\n\t      <p>\n\t\tInstead, I will start with kernel methods within the reproducing kernel Hilbert space (RKHS) framework and talk about how support vector machines are simply kernel methods with {Math.inline`l_\\infty`} error ball constraints.\n\t      </p>\n\t      <h5>Problem Setup: Supervised Machine Learning</h5>\n\t      <p>\n\t\tIn supervised machine learning, we are interested in an interpolation problem. Given many data pairs {Math.inline`(x_1, y_1), \\ldots, (x_n, y_n)`}, we would like to output a new {Math.inline`y`} given a new {Math.inline`x`}. This is exactly interpolation, so the mental picture is something like this:\n\t      </p>\n\t      <p>\n\t\tFor classification, the only difference is that the output is discrete, say {Math.inline`0`} or {Math.inline`1`}. But the problem is still an interpolation problem:\n\t      </p>\n\t      <p>\n\t\tIn general, there are infinitely many solutions to interpolate between finitely many data points. But if we sample fine enough, and if the underlying distribution is somewhat regular, then intuitively we should be able to interpolate between points accurately. And one way of interpolating points is through kernel methods. We can construct an interpolating function by considering linear combinations of kernel functions sitting on the data points.\n\t      </p>\n\t      <p>\n\t\tIn particular, RKHS looks at the space of all functions \"span\" by a positive definite kernel function {Math.inline`k(x, z)`}. One way of constructing an RKHS is first considering the space of all functions  constructed with finite sums of the form {Math.inline`f(x) = \\sum_{i = 1}^m \\alpha_i k(x, z_i)`}, and defining the inner product as {Math.inline`\\langle k(x_i, \\cdot), k(x_j, \\cdot) \\rangle = k(x_i, x_j)`}. Completing the space by including all limits of the finite sums forms an RKHS.\n\t      </p>\n\t      <p>\n\t\tGiven an RKHS {Math.inline`\\mathbb{F}`} with a kernel function {Math.inline`k(x, z)`}, function evaluation can be expressed as a linear operation: {Math.inline`f(x) = \\langle f, k(x, \\cdot) \\rangle`}.\n\t\t{Math.block`\\begin{align} &\\underset{f \\in \\mathbb{F}}{\\text{minimize}} &&\\| f \\|_\\mathbb{F} \\\\ &\\text{subject to} &&f(x_i) = y_i,~\\text{for}~i = 1, \\ldots, n \\end{align}`}\n\t      </p>\n\t      <p>\n\t\tWhat is remarkable is that the optimization can be reduced to a finite-dimentional problem. In particular, we can use duality to obtain the following minimax problem:\n\t\t{Math.block`\\min_{f \\in \\mathbb{F}} \\max_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{2}\\| f \\|_\\mathbb{F}^2 + \\sum_{i = 1}^n \\alpha_i ( y_i - f(x_i) )`}\n\t\tStrong duality holds because the constraints are linear equalities. So we can switch the min-max and minimizes over {Math.inline`f`}. Setting the derivative with respect to {Math.inline`f`} to zero, we obtain:\n\t\t{Math.block`f = \\sum_{i = 1}^n \\alpha_i  k(x, x_i)`}\n\t      </p>\n\t      <p>\n\t\tThis results in a function that is a linear combination of kernel functions sitted on the sampled data.\n\t      </p>\n\t      <p>\n\t\tSubstituting {Math.inline`f`} into the objective function and rearranging, we obtain:\n\t\t{Math.block`\\min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{2} \\alpha^T K \\alpha - \\alpha^T y`}\n\t\tHence, {Math.inline`\\alpha = K^{-1} y`}\n\t      </p>\n\t      <h5>SVM</h5>\n\t      <p>\n\t\t{Math.block`\\begin{align} &\\underset{f \\in \\mathbb{F}}{\\text{minimize}} &&\\| f \\|_\\mathbb{F} \\\\ &\\text{subject to} &&| f(x_i) - y_i | \\leq \\epsilon,~\\text{for}~i = 1, \\ldots, n \\end{align}`}\n\t\tLet us split the error constraint into positive and negative and look at the Lagrangian dual again:\n\t\t{Math.block`\\min_{f \\in \\mathbb{F}} \\max_{\\alpha^+ \\geq 0, \\alpha^- \\leq 0} \\frac{1}{2}\\| f \\|_\\mathbb{F}^2 + \\sum_{i = 1}^n \\alpha^+_i (f(x_i) - y_i - \\epsilon) + \\sum_{i = 1}^n \\alpha^-_i (-f(x_i) + y_i - \\epsilon)`}\n\t      </p>\n\t      <p>\n\t\tLet {Math.inline`\\alpha = \\alpha^+ - \\alpha^-`}, then we have,\n\t\t{Math.block`\\min_{f \\in \\mathbb{F}} \\max_{\\alpha} \\frac{1}{2}\\| f \\|_\\mathbb{F}^2 + \\sum_{i = 1}^n \\alpha_i (f(x_i) - y_i) - \\epsilon \\| \\alpha \\|_1`}\n\t      </p>\n\t      <p>\n\t\tSimilar to exact equality, optimizing over {Math.inline`f`} results in\n\t\t{Math.block`f = \\sum_{i = 1}^n \\alpha_i  k(x, x_i).`}\n\t\tSubstituting {Math.inline`f`} into the objective function and rearranging, we obtain:\n\t\t{Math.block`\\min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{2} \\alpha^T K \\alpha - \\alpha^T y + \\epsilon \\| \\alpha \\|_1`}\n\n\t      </p>\n\t    </Text>\n\t  </Block>\n\t</Layout>\n    )\n}\n"],"sourceRoot":""}